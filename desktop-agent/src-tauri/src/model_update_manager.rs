use crate::backend_communicator::BackendCommunicator;
use crate::app_core::AppCore; 
use crate::storage_manager::StorageManager;
use crate::inference::InferenceEngine;
use crate::StorageManagerArcMutex;
use std::sync::{Arc, Mutex};
use std::time::Duration;
use tauri::{AppHandle, Manager};
use tokio::time::sleep;

// ëª¨ë¸ ì €ì¥ ê²½ë¡œ (lib.rsì™€ ì¼ì¹˜ì‹œí‚´)
const MODEL_DIR: &str = "models"; // ìƒëŒ€ ê²½ë¡œë§Œ ì •ì˜ (OS ê²½ë¡œì™€ ê²°í•©ìš©)
const MODEL_FILENAME: &str = "personal_model.onnx";
const SCALER_FILENAME: &str = "scaler_params.json";

// êµ¬ì¡°ì²´ ì •ì˜: ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ ê°ì²´
// Cloneì´ ê°€ë³ë„ë¡ ì„¤ê³„ (AppHandleì€ ë‚´ë¶€ì ìœ¼ë¡œ Arcì™€ ìœ ì‚¬í•¨)
#[derive(Clone)]
pub struct ModelUpdateManager {
    app_handle: AppHandle,
}

impl ModelUpdateManager {
    // ìƒì„±ì
    pub fn new(app_handle: AppHandle) -> Self {
        Self { app_handle }
    }

    // ì—…ë°ì´íŠ¸ í™•ì¸ ë° ìˆ˜í–‰ (Result<bool> ë°˜í™˜: true=ì—…ë°ì´íŠ¸ë¨)
    // ì´ ë©”ì„œë“œëŠ” 'ë°±ê·¸ë¼ìš´ë“œ ë£¨í”„'ì™€ 'í”„ë¡ íŠ¸ì—”ë“œ ì»¤ë§¨ë“œ' ì–‘ìª½ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    pub async fn check_and_update(&self, token: &str) -> Result<bool, String> {
        // 1. í•„ìš”í•œ State ê°€ì ¸ì˜¤ê¸° (AppHandleì„ í†µí•´ ì ‘ê·¼)
        let communicator = self.app_handle.try_state::<Arc<BackendCommunicator>>()
            .ok_or("BackendCommunicator state not found")?
            .inner().clone();

        // 2. ê²½ë¡œ ì„¤ì •
        let app_data_dir = self.app_handle.path().app_data_dir()
            .map_err(|e| format!("Failed to get app data dir: {}", e))?;
        
        let model_dir = app_data_dir.join(MODEL_DIR);
        if !model_dir.exists() {
            std::fs::create_dir_all(&model_dir).map_err(|e| e.to_string())?;
        }

        let final_model_path = model_dir.join(MODEL_FILENAME);
        let final_scaler_path = model_dir.join(SCALER_FILENAME);

        // 3. ë²„ì „ í™•ì¸ (API í˜¸ì¶œ)
        let info = communicator.check_latest_model_version(token).await
            .map_err(|e| format!("Check version failed: {}", e))?;

        // TODO: ë¡œì»¬ ë²„ì „ê³¼ ë¹„êµ ë¡œì§ ì¶”ê°€ (í˜„ì¬ëŠ” ë¬´ì¡°ê±´ ì§„í–‰)
        // println!("Remote version: {}", info.version);

        // 4. ë‹¤ìš´ë¡œë“œ (ì„ì‹œ íŒŒì¼)
        let temp_model_path = model_dir.join("temp_model.onnx");
        let temp_scaler_path = model_dir.join("temp_scaler.json");

        communicator.download_file(&info.download_urls.model, &temp_model_path, token).await
            .map_err(|e| format!("Download model failed: {}", e))?;
        communicator.download_file(&info.download_urls.scaler, &temp_scaler_path, token).await
            .map_err(|e| format!("Download scaler failed: {}", e))?;

        // 5. Atomic Swap & Reload (Critical Section)
        // ----------------------------------------------------------------
        // [í•µì‹¬ ìˆ˜ì •] AppCoreë¥¼ Lockí•˜ê³  ë‚´ë¶€ì˜ InferenceEngineì„ í†µì§¸ë¡œ êµì²´
        // ----------------------------------------------------------------
        if let Some(app_core_state) = self.app_handle.try_state::<Mutex<AppCore>>() {
            let mut core = app_core_state.lock().map_err(|_| "Failed to lock AppCore")?;

            // 1. ê¸°ì¡´ ì—”ì§„ ì œê±° (ë©”ëª¨ë¦¬ í•´ì œ ë° íŒŒì¼ ë½ í•´ì œ)
            core.inference_engine = None;
            
            // íŒŒì¼ ë½ì´ í’€ë¦´ ì‹œê°„ì„ ì§§ê²Œ ë¶€ì—¬ (ìœˆë„ìš° í™˜ê²½ í•„ìˆ˜)
            std::thread::sleep(Duration::from_millis(100));

            // 2. íŒŒì¼ êµì²´ (ë°±ì—… í›„ ë®ì–´ì“°ê¸°)
            if final_model_path.exists() {
                let _ = std::fs::rename(&final_model_path, final_model_path.with_extension("bak"));
            }
            std::fs::rename(&temp_model_path, &final_model_path).map_err(|e| e.to_string())?;
            std::fs::rename(&temp_scaler_path, &final_scaler_path).map_err(|e| e.to_string())?;

            // 3. ìƒˆ íŒŒì¼ë¡œ ìƒˆ ì—”ì§„ ê°ì²´ ìƒì„±í•˜ì—¬ AppCoreì— ì£¼ì…
            match InferenceEngine::new(
                final_model_path.to_str().unwrap(), 
                final_scaler_path.to_str().unwrap()
            ) {
                Ok(new_engine) => {
                    core.inference_engine = Some(new_engine);
                    println!("âœ… Model updated and reloaded to version {}", info.version);
                    Ok(true)
                },
                Err(e) => {
                    // ìƒˆ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ, ë°±ì—… íŒŒì¼ë¡œ ë³µêµ¬ë¥¼ ì‹œë„í•´ì•¼ í•˜ë‚˜ ì—¬ê¸°ì„œëŠ” ìƒëµí•˜ê³  ì—ëŸ¬ ë°˜í™˜
                    Err(format!("Failed to load new model: {}", e))
                }
            }
        } else {
            Err("AppCore state not found".to_string())
        }
    }
}

pub fn start_update_loop(app_handle: AppHandle) {
    tauri::async_runtime::spawn(async move {
        println!("ğŸš€ Model Update Loop Started.");
        sleep(Duration::from_secs(5)).await;

        // Manager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë£¨í”„ ë‚´ì—ì„œ ì‚¬ìš©)
        let manager = ModelUpdateManager::new(app_handle.clone());

        loop {
            // í† í° ê°€ì ¸ì˜¤ê¸°
            let token_opt = if let Some(storage_mutex) = app_handle.try_state::<StorageManagerArcMutex>() {
                let storage = storage_mutex.lock().unwrap();
                storage.load_auth_token().unwrap_or(None).map(|t| t.0)
            } else {
                None
            };

            if let Some(token) = token_opt {
                // [í•µì‹¬] ë¡œì§ ì¬ì‚¬ìš©: check_and_update í˜¸ì¶œ
                match manager.check_and_update(&token).await {
                    Ok(updated) => {
                        if updated { println!("âœ¨ Background update success."); }
                    },
                    Err(e) => {
                        // ë°±ê·¸ë¼ìš´ë“œì—ì„œëŠ” ì—ëŸ¬ê°€ ë‚˜ë„ ì£½ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹€
                        // eprintln!("Background update check failed: {}", e);
                    }
                }
            }

            sleep(Duration::from_secs(3600)).await;
        }
    });
}