use ndarray::{Array2, Array1};
use ort::session::{Session, builder::GraphOptimizationLevel};
use ort::value::Value; 
use serde::Deserialize;
use std::fs::File;
use std::io::BufReader;
use std::path::{Path, PathBuf};
use std::collections::HashMap;
use std::time::{Instant, Duration};


// 1. JSON ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° êµ¬ì¡°ì²´
// backend/train.pyì—ì„œ ì €ì¥í•œ scaler_params.jsonê³¼ ë§¤í•‘ë¨
#[derive(Debug, Deserialize)]
pub struct ScalerParams {
    pub mean: Vec<f64>,
    pub scale: Vec<f64>,
    // var, n_samples_seenì€ ì¶”ë¡ ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ë¬´ì‹œ
}

// 2. ë¬¸ì„œ ëª…ì‹œëœ íŒë‹¨ ê²°ê³¼ ì—´ê±°í˜•
#[derive(Debug, Clone, PartialEq)]
pub enum InferenceResult {
    Inlier,       // ì •ìƒ (Score > 0.0)
    WeakOutlier,  // ì• ë§¤í•œ ì´íƒˆ (-0.5 < Score <= 0.0)
    StrongOutlier // í™•ì •ì  ì´íƒˆ (Score <= -0.5)
}

// 3. ë©”ì¸ ì¶”ë¡  ì—”ì§„
pub struct InferenceEngine {
    // ONNX Runtime ì„¸ì…˜ (Thread-safeí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ &mut ì ‘ê·¼ í•„ìš”)
    session: Option<Session>, // Optionìœ¼ë¡œ ê°ì‹¸ì„œ Unload(None) ìƒíƒœ í—ˆìš© -> Windows File Lock í•´ê²°
    scaler: ScalerParams,
    
    // Hot-Swapì„ ìœ„í•´ ê²½ë¡œ ê¸°ì–µ
    model_path: PathBuf,
    scaler_path: PathBuf,

    // Local Cache: ì‚¬ìš©ì í”¼ë“œë°±(False Positive ì‹ ê³ ) ê¸°ì–µ ì¥ì†Œ
    // Key: App/Title Token, Value: ë§Œë£Œ ì‹œê°„(TTL)
    local_cache: HashMap<String, Instant>,
}

impl InferenceEngine {
    /// ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ì—¬ ì—”ì§„ ì´ˆê¸°í™”
    pub fn new<P: AsRef<Path>>(model_path: P, scaler_path: P) -> Result<Self, Box<dyn std::error::Error>> {
        let (session, scaler) = Self::load_resources(&model_path, &scaler_path)?;

        Ok(Self {
            session: Some(session), // Someìœ¼ë¡œ ê°ì‹¸ê¸°
            scaler,
            model_path: model_path.as_ref().to_path_buf(),
            scaler_path: scaler_path.as_ref().to_path_buf(),
            local_cache: HashMap::new(), // ì´ˆê¸°ì—” ê¸°ì–µ ì—†ìŒ
        })
    }

    /// [Internal] íŒŒì¼ ë¡œë“œ í—¬í¼ (ì´ˆê¸°í™” ë° ë¦¬ë¡œë“œ ê³µìš©)
    fn load_resources<P: AsRef<Path>>(model_path: P, scaler_path: P) -> Result<(Session, ScalerParams), Box<dyn std::error::Error>> {
        // A. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        let file = File::open(scaler_path)?;
        let reader = BufReader::new(file);
        let scaler: ScalerParams = serde_json::from_reader(reader)?;

        // B. ONNX ëª¨ë¸ ë¡œë“œ
        let session = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .with_intra_threads(1)? 
            .commit_from_file(model_path)?;
        
        Ok((session, scaler))
    }

    // ================================================================
    // Windows File Lock í•´ê²°ì„ ìœ„í•œ Lifecycle ë©”ì„œë“œ
    // ================================================================

    /// Unload: ëª¨ë¸ íŒŒì¼ í•¸ë“¤ í•´ì œ
    /// ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ Sessionì´ Dropë˜ë©´ì„œ OSì—ê²Œ íŒŒì¼ ì œì–´ê¶Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    pub fn unload_model(&mut self) {
        if self.session.is_some() {
            println!("ğŸ”» [InferenceEngine] Unloading model to release file lock...");
            self.session = None; 
        }
    }

    /// Load: íŠ¹ì • ê²½ë¡œì˜ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œ (ì—…ë°ì´íŠ¸ í›„ í˜¸ì¶œ)
    pub fn load_model(&mut self, model_path: &Path) -> Result<(), Box<dyn std::error::Error>> {
        println!("ğŸ”º [InferenceEngine] Loading model from: {:?}", model_path);
        // ìŠ¤ì¼€ì¼ëŸ¬ëŠ” ê¸°ì¡´ ê²½ë¡œ ì¬ì‚¬ìš© (í•„ìš”ì‹œ ì¸ìë¡œ ë°›ë„ë¡ ìˆ˜ì • ê°€ëŠ¥)
        let (session, _) = Self::load_resources(model_path, &self.scaler_path)?;
        self.session = Some(session);
        self.model_path = model_path.to_path_buf();
        Ok(())
    }

    /// Hot-Swap: ì‹¤í–‰ ì¤‘ ëª¨ë¸ íŒŒì¼(ê²½ë¡œ)ì´ ë°”ë€Œë©´ ë‹¤ì‹œ ë¡œë“œ
    /// new_model_path: Some(path)ê°€ ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ ê²½ë¡œë¡œ ëª¨ë¸ì„ êµì²´í•¨. Noneì´ë©´ ê¸°ì¡´ ê²½ë¡œ ì‚¬ìš©.
    /// Hot-Swap: Unload -> Wait -> Reload íŒ¨í„´ ì ìš©
    pub fn reload(&mut self, new_model_path: Option<PathBuf>) -> Result<(), Box<dyn std::error::Error>> {
        println!("ğŸ”„ [InferenceEngine] Hot-Swap Requested...");
        
        let target_model_path = new_model_path.unwrap_or(self.model_path.clone());

        // 1. ì•ˆì „í•œ êµì²´ë¥¼ ìœ„í•´ Unload ë¨¼ì € ìˆ˜í–‰
        self.unload_model();

        // 2. ì ì‹œ ëŒ€ê¸° (OSê°€ íŒŒì¼ í•¸ë“¤ì„ ë†“ì„ ì‹œê°„ í™•ë³´, Windows ì´ìŠˆ ë°©ì§€)
        // std::thread::sleepì€ blockingì´ì§€ë§Œ, ì—…ë°ì´íŠ¸ëŠ” ê°€ë” ì¼ì–´ë‚˜ë¯€ë¡œ í—ˆìš©
        std::thread::sleep(std::time::Duration::from_millis(100));

        // 3. ë¦¬ë¡œë“œ (íŒŒì¼ì´ êµì²´ë˜ì—ˆë‹¤ê³  ê°€ì •)
        // load_resourcesë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ìŠ¤ì¼€ì¼ëŸ¬ì™€ ì„¸ì…˜ ëª¨ë‘ ê°±ì‹ 
        let (new_session, new_scaler) = Self::load_resources(&target_model_path, &self.scaler_path)?;
        
        self.session = Some(new_session);
        self.scaler = new_scaler;
        self.model_path = target_model_path; 
        
        println!("âœ… [InferenceEngine] Hot-Swapped Successfully.");
        Ok(())
    }
    // ================================================================
    // ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€ (Infer, Cache Update)
    // ================================================================

    /// Feedback: ì‚¬ìš©ìê°€ "ë‚˜ ì¼í•˜ëŠ” ì¤‘ì´ì•¼"ë¼ê³  ì‹ ê³ í•˜ë©´ ìºì‹œì— ë“±ë¡
    /// token: í˜„ì¬ í™œì„± ì°½ì˜ ì‹ë³„ì (ì˜ˆ: "Figma")
    /// ttl_hours: ê¸°ì–µ ìœ ì§€ ì‹œê°„ (ë³´í†µ 24ì‹œê°„)
    pub fn update_local_cache(&mut self, token: String, ttl_hours: u64) {
        let expiration = Instant::now() + Duration::from_secs(ttl_hours * 3600);
        self.local_cache.insert(token.clone(), expiration);
        println!("ğŸ§  Local Cache Updated: '{}' is now trusted until {:?}", token, expiration);
    }
    
    /// ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜
    /// input_vector: FeatureExtractorê°€ ë§Œë“  6ì°¨ì› ë²¡í„°
    /// active_token: í˜„ì¬ í™œì„± ì°½ì˜ í† í° (Cache í™•ì¸ìš©)
    pub fn infer(&mut self, mut input_vector: [f64; 6], active_token: Option<String>) -> Result<(f64, InferenceResult), Box<dyn std::error::Error>> {
        
        // Sessionì´ Noneì´ë©´ ì¶”ë¡  ë¶ˆê°€ (Early Return)
        let session = match &mut self.session {
            Some(s) => s,
            None => return Err("Model is unloaded. Cannot infer.".into()),
        };

        // 1. Local Cache Check & Override (ë¬¸ì„œ Phase 5)
        // ì‚¬ìš©ìê°€ í”¼ë“œë°±ì„ ì¤€ í† í°(ì˜ˆ: "YouTube"ë¡œ ê°•ì˜ ë“£ê¸°)ì´ë¼ë©´
        // ë¬¸ë§¥ ì ìˆ˜(0ë²ˆ ì¸ë±ìŠ¤)ë¥¼ ê°•ì œë¡œ 1.0(ë§Œì )ìœ¼ë¡œ ìˆ˜ì •
        if let Some(token) = active_token {
            if let Some(expire_time) = self.local_cache.get(&token) {
                if Instant::now() < *expire_time {
                    // ìºì‹œ íˆíŠ¸! ë¬¸ë§¥ ì ìˆ˜ ê°•ì œ ìƒí–¥
                    input_vector[0] = 1.0; 
                } else {
                    // ë§Œë£Œëœ ê¸°ì–µ ì‚­ì œ
                    self.local_cache.remove(&token);
                }
            }
        }

        // 2. Preprocessing (Standard Scaling)
        let mut scaled_input = Array2::<f32>::zeros((1, 6));
        for i in 0..6 {
            let val = (input_vector[i] - self.scaler.mean[i]) / self.scaler.scale[i];
            scaled_input[[0, i]] = val as f32;
        }

        // 3. Inference
        let input_tensor = Value::from_array(scaled_input)?;
        let inputs = ort::inputs![ "float_input" => input_tensor ]; 
        let outputs = session.run(inputs)?;

        let scores = outputs["scores"].try_extract_tensor::<f32>()?;
        let current_score = scores.1[0] as f64;

        // 4. Rule-based Decision
        let judgment = if current_score > 0.0 {
            InferenceResult::Inlier
        } else if current_score > -0.5 {
            InferenceResult::WeakOutlier
        } else {
            InferenceResult::StrongOutlier
        };

        Ok((current_score, judgment))
    }
}